{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyramid_RoIAlign_TF",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgsIGu_-2Gt_",
        "colab_type": "text"
      },
      "source": [
        "Practice the code of https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeDL_u0CYA3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Feature Pyramid Network Heads\n",
        "\n",
        "def fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn = True, fc_layers_size = 1024):\n",
        "    \"\"\"\n",
        "    Builds the computation graph of the feature pyramid network classifier and regressor heads. \n",
        "    \n",
        "    rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. \n",
        "    feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution.\n",
        "    image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n",
        "    pool_size: The width of the square feature map generated from ROI Pooling. \n",
        "    num_classes: number of classes, which determines the depth of the results. \n",
        "    train_bn: Boolean. Train or freeze Batch Norm layers. \n",
        "    fc_layers_size: Size of the 2 FC layers \n",
        "\n",
        "    Returns:\n",
        "        logits: [batch, num_rois, NUM_CLASSES] classifiers logits (before softmax) \n",
        "        probs: [batch, num_rois, NUM_CLASSES] classifiers probabilities \n",
        "        bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to proposal boxes \n",
        "    \"\"\"\n",
        "\n",
        "    # ROI Pooling\n",
        "    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n",
        "\n",
        "    x = PyramidROIAlign([pool_size, pool_size], name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEh6-f-VTK8t",
        "colab_type": "code",
        "outputId": "697a6300-c330-4985-cb20-49a286b0a5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "def log2_graph(x):\n",
        "    \"\"\"Implementation of Log2. TF doesn't have a native implementation\"\"\"\n",
        "    return tf.log(x) / tf.log(2.0)\n",
        "\n",
        "import keras.engine as KE\n",
        "\n",
        "class PyramidROIAlign(KE.Layer):\n",
        "    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid\n",
        "    \n",
        "    Params:\n",
        "    - pool_shape : [pool_height, pool_width] of the output pooled regions. Usually [7, 7]\n",
        "\n",
        "    Inputs:     # RPN 결과 나온 bounding boxes들 \n",
        "    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized coordinates. Possibly padded with zeros if not enough boxes to fill the array. \n",
        "    - image_meta: [batch, (meta data)] Image details. See compose_image_meta() <-- image의 attribute을 1d로 만들어준다. \n",
        "    - feature_maps: List of feature maps from different levels of the pyramid. Each is [batch, height, width, channels]\n",
        "\n",
        "    Output:\n",
        "    Pooled regions in the shape: [batch, num_boxes, pool_height, pool_width, channels].\n",
        "    The width and height are those specific in the pool_shape in the layer constructor \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pool_shape, **kwargs):       # <-- pool shape를 input으로 받아야 함\n",
        "        super(PyramidROIAlign, self).__init__(**kwargs)\n",
        "        self.pool_shape = tuple(pool_shape)\n",
        "    \n",
        "    def call(self, inputs):         #<-- inputs: boxes, image_meta, feature_maps\n",
        "        # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
        "        boxes = inputs[0]\n",
        "\n",
        "        # Image meta \n",
        "        # Holds details about the image. See compose_image_meta()\n",
        "        image_meta = inputs[1]\n",
        "\n",
        "        # Feature Maps. List of feature maps from different level of the feature pyramid. Each is [batch, height, width, channels]\n",
        "        feature_maps = inputs[2:]\n",
        "\n",
        "        # Assign each ROI to a level in the pyramid based on the ROI area. \n",
        "        y1, x1, y2, x2 = tf.split(boxes, 4, axis = 2)   #<-- [batch, num_boxes, 1] x 4 \n",
        "        h = y2 - y1\n",
        "        w = x2 - x1\n",
        "\n",
        "        # Use shape of first image. Images in a batch must have the same size. \n",
        "        image_shape = parse_image_meta_graph(image_meta)['image_shape'][0]  # image_shape = [H, W, C]\n",
        "\n",
        "        # Equation 1 in the Feature Pyramid Networks paper. Account for the fact that our coordinates are normalized here. \n",
        "        image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)\n",
        "        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))      # normalized -> feature map 사이즈로 sclae 조정  \n",
        "        roi_level = tf.minumum(5, tf.maximum(2, 4 + tf.cast(tf.round(roi_level), tf.int32)))    # roi_level의 min=2, max=5\n",
        "        roi_level = tf.squeeze(roi_level, 2)   # roi_level.shape = [batch, num_boxes], 2 이상 5 이하의 level 정보를 담고 있다.                                                                 \n",
        "\n",
        "        # Loop through levels and apply ROI Pooling to each. P2 to P5       # P2부터 P5까지 돌며 각 level로 mapping된 roi들에 대해 RoI Pooling을 진행   # RoI Align\n",
        "        pooled = []\n",
        "        box_to_level = []\n",
        "        for i, level in enumerate(range(2, 6)):         # 2, 3, 4, 5 <-- FPN layer index\n",
        "            ix = tf.where(tf.equal(roi_level, level))   # 2d tensors. ix = [# of level-matched rois, 2]   true elements의 roi_level에서의 coordinates을 담는다.(ex. [1 0] : 1번째 batch, 0번째 box)\n",
        "            level_boxes = tf.gather_nd(boxes, ix)       # box shape = [batch, num_boxes, (y1, x1, y2, x2)], box중 ix의 위치에 해당하는 box 좌표 가져올 것. \n",
        "\n",
        "            # Box indices for crop_and_resize\n",
        "            box_indices = tf.cast(ix[:, 0], tf.int32)        # 1d tensor. ix의 column[0]의 값 = batch. 즉, 각 level_matched rois가 속하는 batch들의 값을 담는다. \n",
        "\n",
        "            # Keep track of which box is mapped to which level \n",
        "            box_to_level.append(ix)                     \n",
        "\n",
        "            # Stop gradient propagation to ROI proposals        # box에 관한 정보는 RPN이 다룬다. 따라서 regression, classification 등의 head를 학습시키려면 RPN을 freeze 시켜서 변형을 막아야 한다. \n",
        "            level_boxes = tf.stop_gradient(level_boxes)         \n",
        "            box_indices = tf.stop_gradient(box_indices)\n",
        "\n",
        "            # Crop and Resize \n",
        "            # From Mask R-CNN paper: \"We sample four regular locations, so that we can evaluate either max or average pooling. \n",
        "            # In fact, interpolating only a single value at each bin center (without pooling) is nearly as effective.\" <-- roi align\n",
        "            # Here we use the simplified approach of a single value per bin, which is how it's done in tf.crop_and_resize()\n",
        "            # Result: [batch * numb_boxes, pool_height, pool_width, channels]\n",
        "            pooled.append(tf.image.crop_and_resize(\n",
        "                feature_maps[i], level_boxes, box_indices, self.pool_shape, method = 'bilinear'     # level_boxes: 해당 level에 mapping되는 boxes   # box_indices: 1d\n",
        "            ))                      # pooled.shape = [num_level_boxes, crop_height, crop_width, depth]\n",
        "\n",
        "        # Pack pooled features into one tensor \n",
        "        pooled = tf.concat(pooled, axis = 0)      # pooled.shape = [num_boxes, crop_height, crop_width, depth]\n",
        "\n",
        "        # Pack box_to_level mapping into one array and add another column representing the order of pooled boxes\n",
        "        box_to_level = tf.concate(box_to_level, axis = 0)       # box_to_level.shape = [total # of rois, 2] ==> 2: roi의 batch, box 좌표\n",
        "        box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1)                  # box_range.shape = [total # of rois, 1] ==> 1: range(total # of rois)\n",
        "        box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range], axis = 1)    # box_to_level.shape = [total # of rols, 3] ==> 3: roi의 batch, box 좌표, 0~ # total rois\n",
        "\n",
        "        # Rearrange pooled features to match the order of the original boxes        \n",
        "        # Sort box_to_level by batch then box index                               \n",
        "        # TF doesn't have a way to sort by two columns, so merge them and sort. \n",
        "        sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]       # sorting_tensor = 1d tensor. box_to_level.shape = [total # of rois, 3] [:, 0]: batch좌표, [:, 1]: box좌표. batch 정렬 후 box 정렬  \n",
        "        ix = tf.nn.top_k(sorting_tensor, k=tf.shape(                            # ix = 1d tensor. sorting tensor를 내림차순으로 정렬한 것의 역 indices, 즉 오름차순. batch와 box가 낮을 수록 ix의 앞부분 \n",
        "            box_to_level)[0]).indices[::-1]\n",
        "        ix = tf.gather(box_to_level[:, 2], ix)                                  # 그 indices에 해당하는 box_to_level[:, 2]: range(total # of rois)숫자를 뽑음. (original box의 순서)\n",
        "        pooled = tf.gather(pooled, ix)                                          # pooled = [num_boxes, crop_height, crop_width, depth]\n",
        "\n",
        "        # Re-add the batch dimension\n",
        "        shape = tf.concat([tf.shape(boxes)[:2], tf.shape(pooled)[1:]], axis = 0)           # [batch, num_boxes, crop_height, crop_width, depth]\n",
        "        pooled = tf.reshape(pooled, shape)                                  \n",
        "        return pooled\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0][:2] + self.pool_shape + (input_shape[2][-1], )       # [batch + 7 + #channels, num_boxes, 7]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHMFnIXYUafz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Formatting \n",
        "import numpy as np\n",
        "\n",
        "def compose_image_meta(image_id, original_image_shape, image_shape, window, scale, active_class_ids):\n",
        "    \"\"\"\n",
        "    Takes attributes of an image and puts them in one 1D array. \n",
        "\n",
        "    image_id: An int ID of the image. Useful for debugging. \n",
        "    original_image_shape: [H, W, C] before resizing or padding. \n",
        "    image_shape: [H, W, C] after resizing and padding\n",
        "    window: (y1, x1, y2, x2) in pixels. The area of the image where the real image is (excluding the padding)\n",
        "    scale: The scaling factor applied to the original image (float32)\n",
        "    active_class_ids: List of class_ids available in the dataset from which the image came. \n",
        "    Useful if training on images from multiple datasets where not all classes are present in all datasets. \n",
        "    \"\"\"\n",
        "\n",
        "    meta = np.array(\n",
        "        [image_id] +    # size = 1\n",
        "        list(original_image_shape) +    # size = 3\n",
        "        list(image_shape) +     # size = 3\n",
        "        list(window) +  # size = 4 (y1, x1, y2, x2) in image coordinates\n",
        "        [scale] +   # size = 1\n",
        "        list(active_class_ids)  # size = num_classes\n",
        "    )\n",
        "    return meta "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv_wbcpEVqB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Formatting\n",
        "\n",
        "def parse_image_meta(meta):\n",
        "    \"\"\"\n",
        "    Parses an array that contains image attributes to its components. \n",
        "\n",
        "    meta: [batch, meta length] where meta length depends on NUM_CLASSES\n",
        "\n",
        "    Returns a dict of the parsed values. \n",
        "    \"\"\"\n",
        "    image_id = meta[:, 0]\n",
        "    original_image_shape = meta[:, 1:4]\n",
        "    image_shape = meta[:, 4:7]\n",
        "    window = meta[:, 7:11]      # (y1, x1, y2, x2) window of image in pixels \n",
        "    scale = meta[:, 11]\n",
        "    active_class_ids = meta[:, 12:]\n",
        "    return {\n",
        "        \"image_id\" : image_id.astype(np.int32),\n",
        "        \"original_image_shape\" : original_image_shape.astype(np.int32),\n",
        "        \"image_shape\" : image_shape.astype(np.int32),\n",
        "        \"window\" : window.astype(np.int32),\n",
        "        \"scale\" : scale.astype(np.float32),\n",
        "        \"active_class_ids\" : active_class_ids.astype(np.int32)\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww4FEWUVkKeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Formatting \n",
        "\n",
        "def parse_image_meta_graph(meta):\n",
        "    \"\"\"\n",
        "    Parses a tensor that contains image attributes to its componenets. See compose_image_meta() for more details. \n",
        "    \n",
        "    meta: [batch, meta length] where meta length depends on NUM_CLASSES\n",
        "\n",
        "    returns a dict of the parsed tensors\n",
        "    \"\"\"\n",
        "    image_id = meta[:, 0]\n",
        "    original_image_shape = meta[:, 1:4]\n",
        "    image_shape = meta[:, 4:7]\n",
        "    window = meta[:, 7:11]      # (y1, x1, y2, x2) window of image in pixels \n",
        "    scale = meta[:, 11]\n",
        "    active_class_ids = meta[:, 12:]\n",
        "    return {\n",
        "        \"image_id\" : image_id,\n",
        "        \"original_image_shape\" : original_image_shape,\n",
        "        \"image_shape\" : image_shape,\n",
        "        \"window\" : window,\n",
        "        \"scale\" : scale,\n",
        "        \"active_class_ids\" : active_class_ids,\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBd_peJCiOfI",
        "colab_type": "code",
        "outputId": "13f61b31-bff5-437f-c511-a8ae4a0b34df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import tensorflow as tf, os\n",
        "\n",
        "a = tf.constant([[[1,1],[3,6]],[[7,8],[9,9]]])\n",
        "b = \n",
        "b = tf.where(tf.equal(a,3))\n",
        "with tf.Session() as sess:\n",
        "    output = sess.run(b)\n",
        "\n",
        "print(a.shape)      # batch 2, box 2, coord 2\n",
        "\n",
        "print(output)\n",
        "print(type(output))\n",
        "print(output.ndim)\n",
        "print(type(a))\n",
        "\n",
        "c = [1, 2, 3]\n",
        "d = [2]\n",
        "e = tf.concat([c, d], 0)\n",
        "d = e[:,z]\n",
        "print(e)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-96ed78dc5eae>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    b =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}